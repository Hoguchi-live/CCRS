\documentclass[../main.tex]{subfilesubs}
\begin{document}

\subsection{Global architecture of the code}
Here we introduce the relations between the different files and folders of the code.
At the top level there are three folders: \texttt{example/}, \texttt{optimization/} and \texttt{src/}

Folder \texttt{example/} holds a working instance of a key-exchange using the algorithm.
A \textit{bash} script is provided to compile the example.
This script accepts two flags:
	\begin{itemize}
		\item -DVERBOSE which will print some live information on the key-exchange; and
		\item -DTIMING which will output timings for the isogeny steps in \textit{json} format.
	\end{itemize}
This second flag is used to perform optimization on the $l$-primes bounds.

The \texttt{optimization/} folder contains two python scripts.
The \texttt{optimize.py} script loads a \textit{timings.json} file located in subfolder \texttt{files} and created using the -DTIMING flag.
It uses the \textit{GEKKO} optimizer to find the best combination of bounds on the steps for each $l$-primes.
This optimizes the \textit{timing} of the global algorithm while keeping the \textit{keyspace} size above $256$ bits.
The results can then be found in \texttt{files/}.
A plotting script is also available to graph the speed of steps against $l$.
See the third section on results for more details.

Last and most importantly, the \texttt{src/} folder contains all the necessary functions and structures implementing the CRS protocol.
It is once again divided in four subfolders.

In \texttt{EllipticCurves/} one will find all the functions and structures related to curve manipulation.
The \texttt{models.h} file holds the type definition of elliptic curves in different forms and their corresponding points.
All functions related to memory management are located in the \texttt{memory.c/h} files.
The exact structures holding the elliptic curves are discussed in more details in the next section.
The \texttt{arithmetic.c/h} files contain all the mathematical functions directly related to the curves.
For instance the \textit{Montgomery Ladder}, the extraction of random rational torsion points and the model transformation functions are located there.
The \texttt{auxiliary.c/h} files only serve as shortcuts to unavailable FLINT functions.
The \texttt{pretty\_print.c/h} files define convenient debugging functions mainly used to check on the key-exchange.

Folder \texttt{Isogeny/} holds the two algorithms used to compute isogenies between elliptic curves.
The \texttt{radical.c/h} files implement radical isogenies for primes $3$, $5$ and $7$.
Files \texttt{velu.c/h} account for the $\sqrt{}$-Velu algorithm described further in the paper.
Finally \texttt{walk.c/h} holds the two handle functions used to walk in the isogeny graph using the aforementioned algorithms.

The \texttt{Polynomial/} folder contains the binary-tree based multi-evaluation algorithm in \texttt{multieval.c/h}.
The binary-tree structure and functions themselves are defined in file \texttt{binary\_trees.c/h}.
Alongside with \texttt{roots.c/h} hosting a bruteforce square root extraction algorithm.
This was necessary as no root-extraction methods are available for large fields in the FLINT library.

Finally the \texttt{Exchange/} folder hosts everything related to the global structure of the CRS protocol.
The global environment setup is handled in \texttt{setup.c/h} while secret key generation happens in \texttt{keygen.c/h}.
Applying a key to an elliptic curve is done through the single function located in the \texttt{dh.c} file.
Auxiliary files \texttt{info.c/h} are there to print live information during the exchange  and to get timings for optimization.











\subsection{C structures and memory management}
This section will first introduce the different custom types used in the code to handle context and mathematical objects.
We have made the choice to clearly represent in C the objects we use.
To that extent we defined a number of structures representing elliptic curves, their points and other less obvious data in memory.
We follow the GMP/FLINT standard nomenclature identifying types with a "\_t".

The file \texttt{EllipticCurves/models.h} defines the three types of elliptic curve models we use.
These \textit{structs} named respectively \tF{SW_curve_t}, \tF{MG_curve_t} and \tF{TN_curve_t} stand for Short Weierstrass, Montgomery and Tate normal curves.
Inside these \textit{struct} lies a \tF{const fq_ctx_t *} pointer to the field of definition of the curve.
A pointer is used in order to not re-instantiate the whole field structure for every curves.
The multiple \tF{fq_t} parameters respectively named $a$, $b$; $A$, $B$; $b$, $c$ are the respective models' parameters.
The \tF{fmpz_t} parameter $l$ in the definition of type \tF{TN_curve_t} is used to keep track of the order of the point $(0, 0)$ on the curve.
These all strictly follow the definitions found in the first part of this paper.

The \textit{structs} named respectively \tF{SW_point_t} and \tF{MG_point_t} represent points on the respective curves.
These \textit{structs} contain the coordinate of a point on a curve as \tF{fq_t} elements.
Said curve is linked to the point via a pointer of the respective type.

In file \texttt{Polynomials/binary\_tree.h} we define a binary-tree structure used in the fast-multipoint polynomial evaluation.
The \textit{struct} type \tF{fq_poly_btree_t} holds a \tF{fq_ctx_t *}  pointer to keep track of the field of definition of its elements.
It also holds a pointer to the head (or root) cell of the tree.
This decision to distinguish the whole tree from its cells stems from fear of memory management issues.

The cells are represented by the \tF{fq_poly_bcell_t} type.
This structure also holds a pointer to the field $k$ we are working with.
Most importantly, it holds a \tF{fq_poly_t} which is is a polynomial over $k$.
The remaining fields are \tF{fq_poly_bcell_t} pointers to the left and right child of this cell.
Keyword \textit{struct fq\_poly\_bcell\_t} is used to recursively define these children in the type definition.
Both should be initialized to \textit{NULL} to represent a leaf of the tree.

File \texttt{Exchange/setup.h} holds the \tF{lprime_t} structure representing $l$-primes.
A specific structure has been assigned for this task because of the numerous parameters surrounding $l$-primes.
These consists only of an \tF{fmpz_t} representing $l$ and multiple \textit{unsigned int}.
Integers \textit{lbound} and \textit{hbound} account for the bounds on the number of steps allowed for the walk on the $l$-isogeny graph.
It turns out that both upper and lower bounds are equal since we managed to avoid root extraction on the backward direction by using the quadratic twist.
This structure also holds the extension degree $r$ corresponding to the Frobenius eigenvalue's order mod $l$.
Finally it also contains a uint serving as a boolean to allow or disallow backward walking.
This is necessary as some $l$-primes have one eigenvalue of low order but another one of unusably high order.

In this same file, the global configuration type \tF{cfg_t} is defined.
This type holds the public starting curve as an \tF{MG_curve_t}.
Furthermore, it contains the number of $l$-primes to use in the protocol as well as the \tF{lprime_t} array holding them.
The base field and its extensions up to degree $9$  are held in an \tF{fq_ctx_t} array to avoid re-instantiation.
Finally an unsigned integer defined as a seed for the random key generation.
This last step allow for debugging but also for timing tests.

In file \texttt{Exchange/keygen.h} one can find the definition of type \tF{key__t} representing a secret CRS key.
This twisted nomenclature avoids the already defined \tF{key_t} type while remaining as consistent as possible.
A key holds the number of primes used in the protocol, the list of \tF{lprime_t} used and an \tF{fmpz_t} list holding the bounds on the steps to take for each prime.
Note that these lists should all be ordered from the smallest working extension degree to the largest.
The order of the $l$-primes themselves within the same extension degree does not matter.
This is done to avoid downgrading the base field.
This way we only upgrade (embed) the base field once per degree (refer to the \tF{MG_curve_update_field} function  in \texttt{Exchange/dh.h} for details).

Memory management was the first concern of this project and as such was dealt with very rapidly.
Since we are using GMP through FLINT, we decided to use the same \textit{declare, init, set, clear} paradigm.
That way almost all our functions start by a declaration of the variables used.
Followed by an initialization, often using an \tF{fq_ctx_t} field.
Then the data is set and used accordingly then finally the memory is cleared.
This is respected by all of our custom types.
The corresponding management functions are named following the nomenclature \tF{*_init}, \tF{*_set} and \tF{*_clear} where \tF{*} is the type in question.
Some additional practical functions like \tF{MG_curve_set_si} or \tF{fq_poly_bcell_set_right} allow for shortcuts.
Still they were inspired by the FLINT ecosystem.

Most of our custom initialization and clearing functions are just wrappers for FLINT functions.
This offers a safer and more comfortable handling of the many objects in the code.
However there are multiple instances where manual memory allocation was favorable.
This is the case for instance with binary trees, $l$-primes, keys and config types.
The main reason was for allocating memory for arrays of custom data and/or not having to pre-declare objects.
Sometimes both approaches were used.
This can be seen in \texttt{Exchange/keygen.c} with \tF{key_init} initializing a declared key and \tF{key_init_} returning a pointer to an initialized key.








\subsection{Radical isogenies}
In the CRS algorithm, taking $k$ steps in the $l$-isogeny graph is equivalent to computing a chain of isogenies of degree $l$.
As such, we want to find the image curve of a composition
\[
	E\rightarrow E_1\rightarrow E_2\rightarrow \dots\rightarrow E_{k-1}\rightarrow E_k.
\]
Radical formulas allow us to do just that very efficiently for small $l$ primes.
For a given elliptic curve $E$ and a point $P\in E$ of order $l$, Velu's formulas \cite{sqrtVelu} produce expressions for the curve $E/<P>$.
In article \cite{radical} formulas are given to produce a point $Q\in E/<P>$ such that the composition
\[
	E\rightarrow E/<P> \rightarrow E/<Q>
\]
is of degree $l^2$.
Iterating this approach provides rational, low-degree formulas for the target curve found after $k$ steps in the graph.
The only requirement is a point of order $l$ and the initial curve in Tate normal form.
Looking such a point is described in section 3.6.
The explicit formulas transforming a Montgomery curve into its Tate normal model and back were given in part 2.1.
This is done in the code with functions \tF{MG_get_TN} and \tF{TN_get_MG}.
The radical formulas are implemented in file \texttt{isogeny/radical.c}.

Some $l$-th root extraction is required and we use a little trick to only use fast exponentiation.
Let $l$ be a prime equal to $3$, $5$ or $7$.
Assume we want to look for the $l$-th root of element $x\in\FFp$.
In our case, we have $p + 1 = 0$ mod $ 2l$ for these three values, write $p+1 = 2lk$ for some integer $k$.
Setting $e=(p + 1) / 2l$, one finds $(x^e)^l = \pm x$.
We test the correct root via fast exponentiation and return a $l$-th root of $x$.

As an example, if $E$ is in Tate normal form
\[
	Y^2 + (1-c)XY -bY = X^3 -bX^2
\]
for a point of order $5$, which implies $b=c$,  the curve resulting from a single step (using this point) has expression
\[
	Y^2 + (1-b')XY -b'Y = X^3 -b'X^2
\]
with
\[
	b' = \alpha\frac{\alpha^4 + 3\alpha^3 + 4\alpha^2 + 2\alpha + 1 }{\alpha^4 - 3\alpha^3 + 4\alpha^2 - 2\alpha + 1}
\]
where $\alpha$ is a $5$-th root of $b$.
It is immediately understandable why iterating $k$ times over $b$ is several orders of magnitude faster than using Velu's formulas involving torsion-point generation at each step.






\subsection{Montgomery curve arithmetic}

\newcommand{\x}{\textbf{x}}
\newcommand{\funct}[1]{\texttt{\detokenize{#1}}}
We chose to implement  the Montgomery curve arithmetic as described by Craig Costello and Benjamin Smith \cite{montg}.
It is called x-only arithmetic because any finite protective point $P = (X:Y:Z)$ of $E$ can be mapped to a point $\x(P)=(X:Z)=(X/Z:1)\in\PP^1$ if $Z\neq0$ or to $\x(P)=(1:0)$ if $Z=0$. We write $\x(P)=(X_P, Z_P)$.
Any operation on a finite point $P$ is done after discarding its $Y$ coordinate. This coordinate can be recovered later if needed.

\begin{lemma}[Addition and duplication formulas on Montgomery Curves]Let $P,Q\in E(\FF_q)$.  Assume $X_{P-Q}\neq0$.
	 If $P\neq Q$ then  \begin{equation}\label{xaddeq}\left\{\begin{array}{l}
	 	X_{P+Q} = Z_{P-Q}[(X_P-Z_P)(X_Q+Z_Q) + (X_P+Z_P)(X_Q-Z_Q)]^2\\
	 	Z_{P+Q} = X_{P-Q}[(X_P-Z_P)(X_Q+Z_Q) - (X_P+Z_P)(X_Q-Z_Q)]^2
	 \end{array}\right.\end{equation}

 	If $P=Q$ then \begin{equation}\label{xdbleq}\left\{\begin{array}{l}
 		X_{[2]P}=(X_P + Z_P)^2(X_P - Z_P)^2 \\
 		Z_{[2]P}=(4 X_P Z_P)[(X_P - Z_P)^2 + \frac{A+2}{4}(4 X_P Z_P)]
 	\end{array}\right.\end{equation}
\end{lemma}

Using this lemma, we provide three generic pseudo-operations on Montgomery curve points that constitute the framework for x-only arithmetic in our implementation : addition, doubling and scalar multiplication.

These algorithms are called pseudo-operations because they operate over the quotient set of the curve $E$ by the partition
$\lbrace\lbrace P, -P\rbrace, P\in E\rbrace$. e.g. every point is identified with its opposite. This identification is implied in the following paragraphs.

\paragraph{xADD}The pseudo-addition or differential addition \funct{MG_xADD} returns $P+Q$ given $P,Q\in E$ and their difference $P-Q$ using the formula \eqref{xaddeq}.
Our implementation of xADD uses 4 multiplications, 2 squaring,  3 additions and 3 subtractions in $\FF_q$, for a total of  12 base operations over $\FF_q$.
However the point addition xADD requires to first compute the difference $P-Q$, therefore it cannot be used as a generic point addition algorithm but can only be used in the specific context of a differential addition chain.

\paragraph{xDBL}The pseudo-doubling \funct{MG_xDBL} computes $[2]P$ from input $P$.  The cost is 3 additions, 2 squaring, 2 subtractions, 3 multiplications and 1 division in $\FF_q$, for a total of 11 operations. When calling xDBL multiple times for points on the same curve, in a Montgomery Ladder for instance, one could cache or precompute the value $\frac{A+2}{4}$ from equation \eqref{xdbleq} and therefore shave off 1 addition and 1 division from the cost of every subsequent calls to xDBL.

\paragraph{Montgomery Ladder} \funct{MG_ladder} computes the point multiplication $[k]P$ calling xADD and xDBL. Write $k=\sum_{i=0}^{l-1}k_i2^i$. The Montgomery Ladder follows a differential addition chain of length $2l-1$ wherein the difference of consecutive terms is constant. The execution of \funct{MG_ladder} requires $l$ calls to xDBL and $l-1$ calls to xADD. We give the pseudocode of the Montgomery Ladder:
\begin{minted}[frame=lines, linenos, ]{c}
P0 = P
P1 = xDBL(P)
for(int i=l-2; i>=0, i--) {
	if(k[i]==0){
		P1 = xADD(P0, P1, P)
		P0 = xDBL(P0)
	}
	else{
		P0 = xADD(P0, P1, P)
		P1 = xDBL(P1)
	}
}
return P0
\end{minted}
In the specific case of the Montgomery Ladder the two operations xADD and xDBL could be merged into a single function to avoid repeated computations.
Note that the Montgomery ladder can leak a lot of information via side channel attacks but it can be implemented using a conditional constant-time swap to prevent branching or timing attacks for instance.

\subsection{$\sqrt{}$\textit{-Velu} algorithm}

\begin{lemma}
	Let $E:y^2=x^3+Ax^2+x$ be an elliptic curve over $\FF_q$ and $P\in E[l]$ and let $\phi:E\rightarrow E'$ with kernel $\langle P \rangle$.
	The geometry parameter $A'$ of $E' : y^2=x^3+A'x^2+x$ can be computed using the following formulas
	\begin{equation}\label{veluform}\begin{array}{l}
			h_S(X) = \prod_{s\in S}(X-\x([s]P))\text{, where }S=\{1,3, \ldots, l-2\} \\
			d = \left(\frac{A-2}{A+2}\right)^l \left( \frac{h_S(1)}{h_S(-1)}\right)^8\\
			A'=2\frac{1+d}{1-d}
		\end{array}\end{equation}

\end{lemma}
To construct the isogeny given $P$, we implemented two algorithms \funct{KPS} and \funct{xISOG}. We use the improvements described in \cite{sqrtVelu2} and \cite{sqrtVelu} on the classic VÃ©lu formulae to implement \funct{KPS} and \funct{xISOG}.
\paragraph{KPS} The procedure \funct{KPS} splits $S$ in three subsets $I,J,K$, each of size $O(\sqrt{l})$ such that $S = K\cup(I+J)\cup(I-J)$ with a nice structure with regards to the group operation as described in \cite{sqrtVelu2}.
\[\begin{array}{l}I = \{2b(2i+1)|0\leq i<b'\} \\
J = \{2j+1|0\leq j<b\} \\
K=\{4bb'+1, \ldots, l-4, l-2\}\end{array}\]
where $b=\lfloor \sqrt{l-1}/2\rfloor,\, b'=\lfloor (m+1)/4b \rfloor$ if $b>0$ or else $b'=0$.\\
These 3 sets are differential addition chains for they each are in arithmetic progression thus we can use the fast x-only arithmetic xADD to compute each $\x([k]P)$ for $k\in I\cup J\cup K$.
Cost: $O(\sqrt{l})$ calls to xADD

\paragraph{xISOG} This algorithm returns the geometry parameter $A'$ from equation \eqref{veluform}. This computation requires the three sets $\mathcal{I}=\{\x([k]P), k\in I\}$, $\mathcal{J}=\{\x([k]P), k\in J\}$, $\mathcal{K}=\{\x([k]P), k\in K\}$ from \funct{KPS}.
We define the following polynomials \[\begin{array}{l}
	F_0(Z,X)=(Z-X)^2\\
	F_1(Z,X)=-2(Z^2X+(X^2+2AX+1)Z+X)\\
	F_2(Z,X)=(XZ-1)^2\\
	h_\mathcal{I}=\prod_{x_i\in\mathcal{I}}(Z-x_i)\\
\end{array}\]
\begin{itemize}
\item Step 1 : Compute \[\begin{array}{l}
	E_0 = \prod_{x_j\in\mathcal{J}}(F_0(Z,x_j)+F_1(Z,x_j)+F_2(Z,x_j))\\
	E_1 = \prod_{x_j\in\mathcal{J}}(F_0(Z,x_j)-F_1(Z,x_j)+F_2(Z,x_j))\\
\end{array}\]
Cost: $\#\mathcal{J}(7add+6mult)$ over $\FF_q$
\item Step 2 : Lemma 4.9 of \cite{sqrtVelu} expresses $h_{\mathcal{I}\pm \mathcal{J}}$ as the quotient of two resultants. We compute these resultants efficiently using the remainder tree polynomial multievaluation \funct{fq_poly_multieval} described in section \ref{multieval}.
\[\begin{array}{l}
	R_0 = Res_Z(h_\mathcal{I},E_0) = \funct{fq_poly_multieval}(\mathcal{I}, E_0)\\
	R_1 = Res_Z(h_\mathcal{I},E_1) = \funct{fq_poly_multieval}(\mathcal{I}, E_1)\
\end{array}\]
Cost: $O(log(\sqrt{l}))=O(log(l))$ $\FF_q[X]$ multiplications
\item Step 3 : Compute the missing factors from $h_S=h_{\mathcal{I}\pm \mathcal{J}}h_\mathcal{K}$ which is straight forward and deduce $d$ and $A'$ using equation \eqref{veluform} :
 \[\begin{array}{l}
	M_0 = \prod_{x_k\in\mathcal{K}}(1-x_k)\\
	M_1 = \prod_{x_k\in\mathcal{K}}(-1-x_k)\\
	d = \left(\frac{A-2}{A+2}\right)^l \left( \frac{M_0R_0}{M_1R_1}\right)^8\\
	A'=2\frac{1+d}{1-d}
 \end{array}\]
Cost: $O(\sqrt{l})$ multiplications
\end{itemize}













\subsection{Random $l$-torsion point over extensions}
The previous algorithm, $\sqrt{}$-Velu, as well as radical isogenies both need a generator of the kernel to compute the associated isogeny.
Let $\lambda, \mu$ be the eigenvalues of the Frobenius modulo an Elkies prime $l$ and let $r, s$ be their orders and assume $r\leq s$.
Recall in the section on the \textit{group action} that the ideals we use to take steps in the graph are of the form $\af = (\pi -\lambda, l)$ and $\hat{\af} = (\pi -\mu, l)$.
By definition, the associated isogeny verifies
\[
        \ker\phi_{\af} = \left\{P\in E\ |\ \pi P = \lambda P \text{ and } lP = O \right\} = \left\{P\in E[l]\ |\ \pi P = \lambda P  \right\}.
\]
Notice that for a point $P$ in this kernel $\pi^r P = \lambda^r P = P$, hence $P\in E(\FF_{q^r})[l]$.
Now if $s$ and $r$ are coprime, ie $r$ does not divide $s$, then $\ker\phi_{\hat{\af}} \subset  E(\FF_{q^s})[l]\setminus E(\FF_{q^r})[l]$.
The description of $E[l]$ as the sum of the two eigenspaces of $\pi$ thus implies that
\[
       \ker\phi_{\af}  = E(\FF_{q^r})[l].
\]
In this project, we only use primes satisfying the coprimality condition on the orders of the eigenvalues.
We are thus reduced to generating isogeny kernels from $l$-torsion points living in specified extensions.

Looking for a point $P\in E(\FF_{q^r})[l]$ is conceptually simple.
First, precompute the quotient $C := \#E(\FF_{q^r}) / l$.
Then pick a non-trivial random point $Q\in E(\FF_{q^r})$ and set $P := C\cdot Q$.
If $P\neq O$, the point $P$ is in $E(\FF_{q^r})[l]$.
Otherwise pick a new point $Q$.

The probability of hitting such an element is $C/\#E(\FF_{q^r}) = 1/l$ so few tries are needed.
However the crux of the algorithm is the computation of $C\cdot Q$ using the \textit{Montgomery ladder}.
The ladder has complexity $O((A(l, r) + D(l, r))\log{l})$ where $A$ and $D$ represent the complexity of the \funct{MG_xADD} and \funct{MG_xDBL} functions over $\FF_{q^r}$ respectively.
The main issue in practice is the degree of the extension.
While GMP has optimized algorithms, since $\# E(F_{q^r}) \sim q^r$, we are reaching $10^5$ iterations in the ladder for our $q$ of $512$ bits and $r=9$.
Still we found a trick to work in extensions of even degree up to $18$.

Assume $r$ is even and written as $r = 2r'$ and suppose we want to find  a point $P\in F : = E(\FF_{q^r})[l]\setminus E(\FF_{q^{r'}})[l]$ (this is the situation above for eigenvalue $\mu$).
The restriction of $\pi^{r'}$ to $E(\FF_{q^r})[l]$  has eigenvalues $1$ and $-1$.
It follows that points in $F$ are characterized by equation
\[
	\pi^{r'} P = -P.
\]
Writing $P=(x, y)$, this is equivalent to saying that $x\in \FF_{q^{r'}}$ and $y\in \FF_{q^{r}}\setminus\FF_{q^{r'}}$.
Using $x$-only arithmetic on Montgomery curves, this means that we can work in an extension of degree $r/2$ instead of $r$.

The functions \funct{MG_point_rand_ninfty} and \funct{MG_point_rand_ninfty_nsquare} return a random non-trivial point with $x$ in $\FF_{q^{r}}$ and $\FF_{q^{r//2}}$ respectively.
Following is the pseudo-code for \funct{MG_point_rand_ninfty}.
\begin{minted}[frame=lines, linenos, ]{c}
is_square = 0
while(is_square == 0) {
	x = random element in Fq^r
	tmp = x^3 + Ax^2 + x
	is_square = (is tmp a square in Fp^r?)
}
return [x, 1] //Montgomery point
\end{minted}
And the \funct{MG_point_rand_ninfty_nsquare} version.
\begin{minted}[frame=lines, linenos, ]{c}
is_square = 1
while(is_square == 1) {
	x = random element in Fq^r' // r = 2r'
	tmp = x^3 + Ax^2 + x
	is_square = (is tmp a square in Fp^r'?)
}
return [x, 1] (Montgomery point)
\end{minted}



\subsection{Multi-evaluation using remainder trees\label{multieval}}
To compute the resultant in the $\sqrt{}$-Velu algorithm, a binary-tree approach was used.
Notice first that evaluating a polynomial $P\in k[X]$ at a point $\alpha\in k$ is equivalent to computing the image of $P$ in $k[X]/(X-\alpha)$.
Suppose now that we want to evaluate a polynomial $P$ at multiple roots $\alpha_0, \cdots,\alpha_n$.
We first split the roots into two groups $I$, $J$ and compute $P$ mod $\prod_{i\in I} (X-\alpha_i)$ and $P$ mod $\prod_{j\in J} (X-\alpha_j)$.
We iterate the procedure until we get to to reducing $P$ mod a single root.
The only problem is computing all the products efficiently.
This is done by building a binary tree containing the different products from the bottom up using \funct{remainderCell} in \texttt{Polynomials/multieval.c}.
This step has complexity $O(\mathbf{M}(n)\log(n))$ with $\mathbf{M}$ a bound function on the polynomial multiplication complexity as there is a multiplication for each cell and the tree has depth $\log_2(n)$.
After that, the remainders can be computed in $\log_2(n)$ reductions.
The resulting complexity is quasi-linear in the number of points, as opposed to quadratic for a naive Horner's scheme.

The algorithm is presented here, using the notations from above and setting $Q(i, j) :=\prod_{k=i}^j (X-\alpha_k)$ for $1\leq i\leq j\leq n$.
\begin{enumerate}
	\item Set the root of the tree as $P$ mod $Q(1, n)$.
	\item Add a left child $P$ mod $Q(1, n//2)$ and a right child $P$ mod $Q(n//2 + 1, n)$.
	\item Recursively repeat step $1$ and $2$ on the children until reaching a depth of $\log_2n$.
	\item The leafs of the tree now contain the results of the evaluations.
\end{enumerate}
A detailed analysis of the algorithm done in \cite{multi} gives the following complexity for the whole algorithm.
\begin{lemma}
	If $P\in k[X]$ is of degree $l$, its evaluation at $n$ distinct roots runs in
	\[
		2\mathbf{M}(l) + 4\mathbf{M}(n)\log(n) + O(l + \mathbf{M}(n)\log\log(n))
	\]
operations in $k$.
\end{lemma}



\subsection{Exchanging keys}
Suppose Alice and Bob want to initiate a key-exchange using this CRS protocol.
Both will first setup their environment using the functions in \texttt{Exchange/setup.c}.
At this point they both have access to the global public parameters such as the base field, the base curve, the trace of the curve and the $l$-primes.

In order to actually share a secret, Alice and Bob first have to generate their private keys.
This is done through the \funct{keygen} function in \texttt{Exchange/}.
The \textit{seed} and \textit{state} parameters can be used for reproducibility.
The function takes for every prime $l$ in the configuration a number of steps to take in the $l$-isogeny graph.
This number can be negative when walking on the quadratic twist of the curve is possible, i.e. when both eigenvalues have small orders.
In the end, Alice and Bob both get a random list of steps to take in the different $l$-isogeny graph.
To generate their public key, they must apply this key to the public base curve hardcoded in the \texttt{setup.h} file and available in the \funct{cfg_t} structure.
A call to \funct{apply_key} located in \texttt{Exchange/dh.c} is all that is necessary.
To minimize the time spent applying a key, we chose to group $l$-primes according to the degree of their eigenvalues in ascending order.
That way we only have to change our working field $\FF_{q^r}$ to $\FF_{q^{r+1}}$ once per degree.
At this stage Alice and Bob both posses a curve that they publicly exchange.
In practice this would be done by exchanging the associated $j$-invariant, not the curve itself.

To compute the shared secret, both parties use the function \funct{apply_key} with their private key on their counterpart's public key.
The group action highlighted in the theoretical section and its properties ensure that the shared secret is the same for both Alice and Bob.
More precisely, the shared secret are curves (not necessarily the same) member of the same class of isomorphism (they have the same $j$-invariant).




\end{document}
